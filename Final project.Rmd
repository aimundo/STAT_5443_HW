---
title: STAT 5443, FINAL PROJECT
author: Ariel Mundo
date: "`r Sys.Date()`"
output:
    prettydoc::html_pretty:
    pdf_document: 
    theme: cayman
    css: styles.css
---
<style type="text/css">

p {
   font-size: 18px;
}
code.r{
  font-size: 14px;
}
pre {
  font-size: 18px
}
</style>

## Sparse Classification 

### Comparing penalized methods for High-dimensional Classification

We would like to compare the performance of several modern regression methods including penalized regression methods such as LASSO and Elastic Net regression with 10-fold cross-validation. 

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
library(prettydoc)
```

Library hdrm was installed following the directions from:
https://github.com/pbreheny/hdrm


```{r, loading data}

library(hdrm)
library(glmnet)
library(ggplot2)
library(tidyverse)
library(coefplot)
downloadData(Golub1999)
attachData(Golub1999)
```
http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/#computing-penalized-logistic-regression



```{r, splitting the data}
n=nrow(X)

#split in train and test sets
set.seed(1234)
train_rows <- sample(1:n, n/2)
X.train <- X[train_rows, ]
X.test <- X[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]

```


https://daviddalpiaz.github.io/stat432sp18/hw/hw07-soln.html
https://stackoverflow.com/questions/20572919/finding-misclassification-rate-in-r-loan-default

```{r, ELASTIC NET, LASSO AND RIDGE}
#cross validation

alpha=0.5
#Using cross validation for best lambda in all models
fit.elastic_net<-cv.glmnet(X.train,y.train,family="binomial",type.measure = "class",alpha=alpha)
fit.lasso<-cv.glmnet(X.train,y.train,family="binomial",type.measure = "class",alpha=1)
fit.ridge<-cv.glmnet(X.train,y.train,family="binomial",type.measure = "class",alpha=0)
par(mfrow=c(2,2))
plot(fit.elastic_net, main="Elastic net")
plot(fit.lasso, main="Lasso")
plot(fit.ridge, main="Ridge")
par(mfrow=c(1,1))

#Solution paths
par(mfrow=c(2,2))
plot(fit.elastic_net$glmnet.fit,xvar="lambda", main="Elastic net")
plot(fit.lasso$glmnet.fit,xvar="lambda",main="LASSO")
plot(fit.ridge$glmnet.fit,xvar="lambda",main="Ridge")

```

The solution paths are very crowded for the elastic net and ridge regression. Non-zero coefficients will be extracted and sorted in descending order.

```{r,COEFFICIENT PLOTS}
#making dataframes from the non-zero coefficients
coef.enet<-coef(fit.elastic_net,s="lambda.min")
coef.enet <- data.frame(
  features = coef.enet@Dimnames[[1]][ which(coef.enet != 0 ) ], 
  coefs    = coef.enet              [ which(coef.enet != 0 ) ]  
)

coef.lasso<-coef(fit.lasso,s="lambda.min")
coef.lasso <- data.frame(
  features = coef.lasso@Dimnames[[1]][ which(coef.lasso != 0 ) ], 
  coefs    = coef.lasso              [ which(coef.lasso != 0 ) ]  
)

coef.ridge<-coef(fit.ridge,s="lambda.min")
coef.ridge <- data.frame(
  features = coef.ridge@Dimnames[[1]][ which(coef.ridge != 0 ) ], 
  coefs    = coef.ridge              [ which(coef.ridge != 0 ) ]  
)

#Sorting in decreasing order

#Because the number of coefficients for Ridge Regression is too high (~7130) only the first 50 will be used for plotting purposes.
coef.ridge_sorted<-coef.ridge %>% arrange(-coefs)
coef.ridge_sorted<-coef.ridge_sorted[1:30,]


#making plots with values in descending order

ggplot(coef.enet,aes(x=coefs,y=reorder(features,coefs)))+geom_point(size=2)+ggtitle('Elastic Net factors')
ggplot(coef.lasso,aes(x=coefs,y=reorder(features,coefs)))+geom_point(size=2)+ggtitle('LASSO factors')
ggplot(coef.ridge_sorted,aes(x=coefs,y=reorder(features,coefs)))+geom_point(size=2)+ggtitle('Ridge factors')
#coefplot(fit.elastic_net,lambda='lambda.min',sort="magnitude")
#coefplot(fit.lasso,lambda='lambda.min',sort="magnitude")
#coefplot(fit.ridge,lambda='lambda.min',sort="magnitude")
```

```{r,FITTING MODELS TO DATA}


#Fitting the models to the training data

mod.enet<-glmnet(X.train,y.train,alpha=0.5,family = "binomial",lambda=fit.elastic_net$lambda.min)
mod.lasso<-glmnet(X.train,y.train,alpha=1,family = "binomial",lambda=fit.lasso$lambda.min)
mod.ridge<-glmnet(X.train,y.train,alpha=0,family = "binomial",lambda=fit.ridge$lambda.min)


#Number of significant (non-zero) coefficients per model for the training set


length(coef.enet)
length(coef.lasso)
length(coef.ridge)
```

LASSO has the lowest number of coefficients (17), followed by elastic net (32) and finally Ridge (7130).

The predictions will be made using the package <code>caret</code>

```{r,PREDICTIONS}


#Predictions on training data
prob.enet<-mod.enet %>% predict(newx=X.test,s=fit.elastic_net$lambda.min,type="response")
prob.lasso<-mod.lasso %>% predict(newx=X.test,s=fit.lasso$lambda.min,type="response")
prob.ridge<-mod.ridge %>% predict(newx=X.test,s=fit.ridge$lambda.min,type="response")

contrasts(y)

#because from the contrast matrix level 0 is ALL and level 1 is AML, the probabilities will be recoded accordingly. The predictions with probability above 0.5 will be considered AML and if below 0.5 will be considered ALL.
pred.class.enet<-ifelse(prob.enet>0.5,"AML","ALL")
pred.class.lasso<-ifelse(prob.lasso>0.5,"AML","ALL")
pred.class.ridge<-ifelse(prob.ridge>0.5,"AML","ALL")


confusionMatrix(data=as.factor(pred.class.enet),reference=y.test)
confusionMatrix(data=as.factor(pred.class.lasso),reference=y.test)
confusionMatrix(data=as.factor(pred.class.ridge),reference=y.test)

table(y.test,pred.class.enet)
table(y.test,pred.class.lasso)
table(y.test,pred.class.ridge)
#pred.class.enet<-ifelse(prob.enet>0.5,"ALL","AML")
#pred.class.lasso<-ifelse(prob.lasso>0.5,"AML","ALL")
#pred.class.ridge<-ifelse(prob.ridge>0.5,"AML","ALL")

#Misclassification error

1-mean(pred.class.enet==y.test)
1-mean(pred.class.lasso==y.test)
1-mean(pred.class.ridge==y.test)
```

